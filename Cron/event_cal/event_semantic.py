#!/usr/bin/env python
# -- coding: utf-8 --
# @Time : 2020/2/12 21:52
# Author : Hu
# File : event_semantic.py

import sys
import re
import json
import emoji
import jieba
from gensim import corpora, models
import random
sys.path.append("../../")
from Config.db_utils import es, pi_cur, conn
import time
from Config.time_utils import date2ts,today
WEIBO_NUM = 100000
cursor = pi_cur()


#å¯¹å¾—åˆ°çš„å¾®åšæ–‡æœ¬è¿›è¡Œå»é™¤é“¾æ¥ã€ç¬¦å·ã€ç©ºæ ¼ç­‰æ“ä½œ
def weibo_move(data):
    if len(data):
        cut_list = []
        stopwords = set([line.strip() for line in open('stop_words.txt', 'r', encoding='utf-8').readlines()])
        for item in data:
            result = Weibo_utils()
            result.remove_c_t(item)
            text = result.remove_nochn(item)
            cutWords = [k for k in jieba.cut(text) if k not in stopwords and k!=' ']
            cut_list.append(cutWords)
        return cut_list
    else:
        raise Exception("æ— å¾®åšå†…å®¹")


class Weibo_utils:
    def __init__(self):
        self.re_comment = re.compile("å›å¤@.*?:")   # åŒ¹é…è¯„è®ºé“¾
        self.re_link = re.compile('http://[a-zA-Z0-9.?/&=:]*')   # åŒ¹é…ç½‘å€
        self.re_links = re.compile('https://[a-zA-Z0-9.?/&=:]*')   # åŒ¹é…ç½‘å€
        self.re_text = re.compile(r'[^\u4e00-\u9fa5^ ^a-z^A-Z]')   # åŒ¹é…ä¸­æ–‡ç©ºæ ¼ä¸è‹±æ–‡

    # åˆ é™¤è½¬å‘è¯„è®ºé“¾
    def remove_c_t(self,text):
        text = self.re_comment.sub("", text)   # åˆ é™¤è½¬å‘è¯„è®ºé“¾
        text = self.re_link.sub("", text)
        text = self.re_links.sub("", text)
        text = text.split("//@",1)[0]
        text = text.strip()
        if text in set(['è½¬å‘å¾®åš','è½‰ç™¼å¾®åš','Repost','repost']):
            text = ''
        return text

    # ç§»é™¤éä¸­è‹±æ–‡åŠç©ºæ ¼
    def remove_nochn(self, text):
        text = self.re_text.sub("", text)
        return text


# å¤„ç†è¾“å…¥æ•°æ®
def data_process(data):
    if len(data) > WEIBO_NUM:
        data = random.choices(data, k=WEIBO_NUM)
    # é¢„å¤„ç†,åˆ†è¯
    cut_list = weibo_move(data)
    dictionary = corpora.Dictionary(cut_list)
    corpus = [dictionary.doc2bow(text) for text in cut_list]
    tfidf = models.TfidfModel(corpus)
    corpus_tfidf = tfidf[corpus]
    return corpus_tfidf, dictionary


# äº‹ä»¶LDAèšç±»
def lda_analyze(corpusTfidf, dictionary, num_topics=10, iterations=50, workers=6, passes=1):
    lda_multi = models.ldamulticore.LdaMulticore(corpus=corpusTfidf, id2word=dictionary, num_topics=num_topics, \
                                                 iterations=iterations, workers=workers, batch=True,
                                                 passes=passes)  # å¼€å§‹è®­ç»ƒ
    result = lda_multi.show_topics()
    json_data = {}
    for i in result:
        json_data[i[0]] = {}
        item = i[1].split('+')
        probability = [p.split('*')[0].strip() for p in item]
        word = [w.split('*')[1].strip().strip('"') for w in item]
        json_data[i[0]]['æ¦‚ç‡'] = probability
        json_data[i[0]]['ä¸»é¢˜è¯'] = word
        # print(word)
    return json_data


# äº‹ä»¶è¯­ä¹‰åˆ†æ
def event_semantic(e_id, e_name, data, thedate):
    corpus_tfidf, dictionary = data_process(data)
    result = lda_analyze(corpus_tfidf, dictionary, num_topics=5)
    result = json.dumps(result)
    timestamp = date2ts(thedate)
    es_id = timestamp + e_id
    # sql = "insert into Event_Semantic set es_id=%s,e_id=%s,e_name=%s,topics=%s,timestamp=%s,into_date=%s" % (es_id,e_id,e_name,result,timestamp,thedate)
    sql = "insert into Event_Semantic values(%s,%s,%s,%s,%s,%s)"
    val = [(es_id,e_id,e_name,result,timestamp,thedate)]
    try:
        n = cursor.executemany(sql, val)
        print("insert %d success" % n)
        conn.commit()
    except:
        print('å‡ºç°æ•°æ®åº“é”™è¯¯')



def main():
    st = time.time()
    sql = 'select text from Information'
    cursor.execute(sql)
    result = cursor.fetchall()
    data = []
    for i in result:
        data.append(i['text'])
    print(len(data))
    # data = ['ç½®é¡¶ç–«æƒ…è”“å»¶ï¼Œé¢å¯¹ç”Ÿæ´»ç»™æˆ‘ä»¬å¦‚æ­¤ä¸¥è‚ƒçš„æé—®ï¼Œåº”è¯¥å¦‚ä½•å›ç­”ï¼Ÿæˆ‘ä»¬ä»¥#VOGUEä¸‰æœˆå·# @æå®‡æ˜¥ å°é¢å‘Šè¯‰å¤§å®¶ï¼Œåªè¦å¿ƒé‡Œä¿å­˜çƒ­çˆ±ï¼Œå¸Œæœ›ä¾¿ä¸å¯æ‘§æ¯ã€‚å’Œæ‰€æœ‰ä¸­å›½äººä¸€æ ·é¢ä¸´å‘½è¿æ–°è€ƒéªŒçš„æå®‡æ˜¥ï¼Œå†³å®šä»¥æ­Œå”±æ¥å›åº”ï¼Œå¥¹å†™ä¸‹å¿ƒç³»ç–«åŒºçš„ã€Šå²å²å¹³å®‰ã€‹ï¼Œå§”å©‰æ¸©æƒ…ä¸­é€ç€åˆšå¼ºï¼Œå¯å‘æ¯ä¸ªäººä»¥è‡ªå·±çš„æ–¹å¼å»ç»§ç»­ã€‚å¯¹éŸ³ä¹ã€æ—¶å°šã€è‰ºæœ¯ã€åˆ›æ„éƒ½å……æ»¡çƒ­çˆ±çš„æå®‡æ˜¥ï¼Œå¯¹æœªæ¥è¿˜æœ‰å¾ˆå¤šæ¢¦æƒ³å’Œæ†§æ†¬ï¼Œæˆ‘ä»¬å’Œå¥¹ä¸€æ ·ç›¸ä¿¡ï¼Œ#æœ‰åšæŒæœ‰å¸Œæœ›#ã€‚ğŸ’ª #VOGUEæˆ˜ç–«è¿›è¡Œæ—¶#  æ‘„å½±ï¼šNick Knight é€ å‹ï¼šDaniela Paudice', 'å¦‚æœå¯ä»¥å’Œæœªæ¥çš„ä½ é€šä¸€ä¸ªç”µè¯ï¼Œä½ æƒ³å’Œä»–ï¼ˆå¥¹ï¼‰è¯´ä»€ä¹ˆï¼Ÿ#é™ˆé£å®‡æƒ…äººèŠ‚ç”µè¯æ‰“ç»™è°# ï¼Ÿæƒ³çŸ¥é“ä»–è¯´äº†ä»€ä¹ˆå—ï¼Ÿæ•¬è¯·æœŸå¾… 2æœˆ14æ—¥æ­£å¼ä¸Šçº¿çš„èŠ­èç å®ç”µå­åˆŠâ€œé™ˆé£å®‡ 2020çš„æœªçŸ¥æ¥ç”µâ€ï¼é¢„å”®å·²å¼€å¯ï¼Œè¿™ä¸ªéå¸¸æ—¶æœŸçš„0214æƒ…äººèŠ‚â€œå®‡â€çˆ±åŒè¡Œï¼Œæ„¿è¿™ä»½æ¸©æš–é©±æ•£å¯’å†·å’Œé˜´éœ¾ã€‚@é™ˆé£å®‡Arthur @é™ˆé£å®‡å·¥ä½œå®¤ @é™ˆé£å®‡å…¨å›½åæ´ä¼š î˜§é™ˆé£å®‡ç å®//@Pomellatoå®æ›¼å…°æœµ']
    event_semantic(3,'é¦™æ¸¯äº‹ä»¶',data,today())
    et = time.time()
    print(et-st)


if __name__ == '__main__':
    main()
